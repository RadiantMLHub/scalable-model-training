{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a303c34-02dd-49a6-a8a1-e8e6c0538788",
   "metadata": {},
   "source": [
    "## LandCoverNet Data Preparation\n",
    "\n",
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "This tutorial delves into building a scalable model on the LandCoverNet dataset.\n",
    "\n",
    "This portion of the tutorial is focused on developing a semantic segmentation model for LandCoverNet data\n",
    "Here:\n",
    "\n",
    "1. We will inspect the source imagery for the labels we have\n",
    "\n",
    "2. We will process the source imagery in parallel using Dask\n",
    "\n",
    "3. We will select the labels and filtered source images from Dask to be loaded \n",
    "\n",
    "4. We will save the images and associated labels data as a `pickle` file ('.pkl') on our directory to be loaded for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c0d42f-2015-4cba-8ea9-d50afbc764cb",
   "metadata": {},
   "source": [
    "#### Store your MLHub API Developer Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4153b89b-440d-4d9d-a209-e5f37b0a5648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "MLHub API Key:  ································································\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "MLHUB_API_KEY = getpass.getpass(prompt=\"MLHub API Key: \")\n",
    "MLHUB_ROOT_URL = \"https://api.radiant.earth/mlhub/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03050794-0493-4981-bed3-7a4100120db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac\n",
    "import os\n",
    "import itertools as it\n",
    "import pystac_client\n",
    "import requests\n",
    "import shapely.geometry\n",
    "from shapely.geometry import mapping, shape\n",
    "import rioxarray\n",
    "from pystac import Item\n",
    "from typing import List, Tuple\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import stackstac\n",
    "import rasterio as rio\n",
    "import rasterio.plot\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import dask\n",
    "import dask_gateway\n",
    "from pystac.item_collection import ItemCollection\n",
    "from pystac.extensions.eo import EOExtension\n",
    "\n",
    "import contextlib\n",
    "from pystac.extensions.label import LabelExtension, LabelRelType\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \"Creating an ndarray from ragged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85490b75-fb5c-4789-a880-e05d6c5517b5",
   "metadata": {},
   "source": [
    "#### Instantiate an instance of the MLHub API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2703906b-fbfb-4a9c-86f0-78273b9ebb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlhub_client = pystac_client.Client.open(\n",
    "    url=MLHUB_ROOT_URL,\n",
    "    parameters={\"key\": MLHUB_API_KEY},\n",
    "    ignore_conformance=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13feeb77-08d9-4ca3-8287-653befa9ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = \"/home/jovyan/PlanetaryComputerExamples\"\n",
    "if not os.path.isdir(f\"{tmp_dir}/landcovnet/labels\"):\n",
    "    os.makedirs(f\"{tmp_dir}/landcovnet/labels\") #create folder for labels to be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fcd2d5-c7a7-4ea6-b3c5-30964152540e",
   "metadata": {},
   "source": [
    "#### Loading the source imagery\n",
    "\n",
    "The esip-summer-2021-geospatial-ml tutorial was helpful in this task, which can be found [here](https://github.com/TomAugspurger/esip-summer-2021-geospatial-ml/blob/main/segmentation-model.ipynb)\n",
    "\n",
    "It was particularly useful for loading the STAC items and Sentinel-2 scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a62dc5-7197-4842-ae8f-bc165169a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for data in collection file\n",
    "catalog = pystac.read_file(\n",
    "    tmp_dir+\"/landcovnet/labels/ref_landcovernet_v1_labels/collection.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6680af86-7399-460b-944f-57b6f51e62c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:32979': None,\n",
       " 'tcp://127.0.0.1:36787': None,\n",
       " 'tcp://127.0.0.1:37301': None,\n",
       " 'tcp://127.0.0.1:45593': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client.run(lambda: warnings.filterwarnings(\"ignore\", \"Creating an ndarray from ragged\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09af238c-c830-48cd-9f6e-3c99cc681bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1007756f-3df8-461d-a63a-d179eaaef39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway = dask_gateway.Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options[\"worker_cores\"] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b35749-08a7-4f70-be37-8d0e6bd80c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.shutdown() #use this if you want to shutdown dask client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd8aac1-1c64-496e-8980-0ac6ad64eb94",
   "metadata": {},
   "source": [
    "Here, we will select `n` labels with their respective source images for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0c6e5cd-e912-4d92-a107-9bb24fa71883",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = catalog.get_item_links() #links from the catalog\n",
    "label_items = [link.resolve_stac_object().target for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e317e5ca-be19-48b7-91a3-65502ff1f598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e3835b-5c44-489a-b4a2-da953eb031ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cloud_cover(img_arr: np.ndarray) -> int:\n",
    "        \n",
    "    \"\"\"Takes a chip cloud cover band and returns the integer score by dividing the sum of normalized values by the chip area (HxW)\n",
    "    \n",
    "    Args:\n",
    "    img_arr: np.ndarray - 2d array of cloud cover mask\n",
    "    \n",
    "    Returns:\n",
    "    arr_cc: int - integer value of cloud cover score\n",
    "    \n",
    "    \"\"\"\n",
    "    CHIP_AREA = 256 * 256\n",
    "    arr_filled = np.nan_to_num(img_arr)\n",
    "    arr_norm = arr_filled / 100\n",
    "    arr_sum = arr_norm.sum()\n",
    "    arr_cc = arr_sum / CHIP_AREA * 100\n",
    "    return int(arr_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa8adef1-2779-407c-8714-41f49f5391cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_date(id_arr: np.ndarray) -> int:\n",
    "    \n",
    "    \"\"\"Takes a 2d array of source Item IDs for a quarter, and returns median date \n",
    "    \n",
    "    Args: id_arr: np.ndarray - 2d array of string values for source Item IDs\n",
    "    \n",
    "    Returns:\n",
    "    median_date: int - the calculated median date value for input array\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dates = [int(s[-8:]) for s in id_arr]\n",
    "    dates.sort()\n",
    "\n",
    "    n = len(dates)\n",
    "    \n",
    "    # case in which multiple items returned\n",
    "    if n > 1:\n",
    "        if n % 2 == 0:\n",
    "            mid = int(n / 2)\n",
    "        else:\n",
    "            mid = int((n + 1) / 2)\n",
    "        median_date = dates[mid]\n",
    "    # base case there is only one source item\n",
    "    elif n == 1:\n",
    "        median_date = dates[0]\n",
    "    # base case there are no source items\n",
    "    else:\n",
    "        median_date = 0\n",
    "        #print('No dates returned from search criteria')\n",
    "        \n",
    "    return median_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0e97899-26a4-4443-abd8-24918bee75bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_quarter_items(cc_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"Takes a dataframe of source Items with metadata and filters on ranked cloudcover by quarter/season\n",
    "    \n",
    "    Args:\n",
    "    cc_df: pd.DataFrame - unfiltered dataframe\n",
    "    \n",
    "    Returns:\n",
    "    filtered_df: pd.Dataframe - filtered dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # assigns quarter and rank by quarter\n",
    "    cc_df['date_time'] = pd.to_datetime(cc_df['date_time'])\n",
    "    cc_df['quarter'] = cc_df['date_time'].dt.quarter\n",
    "    cc_df['rank'] = cc_df.groupby(\"quarter\")[\"cloud_cover\"].rank(method=\"min\", ascending=True)\n",
    "    # print(cc_df.groupby('quarter').size())\n",
    "\n",
    "    id_prefix = cc_df.iloc[0]['id'][:-8]\n",
    "    median_dates = []\n",
    "\n",
    "    # filters DataFrame on rank\n",
    "    min_cc_df = cc_df[cc_df['rank']==1]\n",
    "    # print(min_cc_df.groupby('quarter').size())\n",
    "\n",
    "    # for each quarter in year, get the median date of source items\n",
    "    for i in range(1, 5):\n",
    "        quarter_df = min_cc_df[min_cc_df['quarter']==i]\n",
    "        quarter_median_date = get_median_date(quarter_df['id'].values)\n",
    "        quarter_median_id = id_prefix + str(quarter_median_date)\n",
    "        print(f'The median quarter date is: {quarter_median_id}')\n",
    "        median_dates.append(quarter_median_id)\n",
    "\n",
    "    # filter the ranked DataFrame by median date\n",
    "    filtered_df = min_cc_df[min_cc_df['id'].isin(median_dates)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "225d596e-d9af-4c5f-a507-5aff263f293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_min_cloud_cover(item_list: List[Item]) -> ItemCollection:\n",
    "    \n",
    "    \"\"\"Takes a list of source Items and returns a single chip per season\n",
    "    ranked by the minimum cloud cover from eo:cloud_cover property\n",
    "    \n",
    "    Args:\n",
    "    item_list: List[Item] - iterable of source Items returned from search\n",
    "    \n",
    "    Returns:\n",
    "    ItemCollection - STAC Iterable containing Items filtered by cloud cover\n",
    "    \"\"\"\n",
    "    \n",
    "    # constructs a DataFrame of each source item properties\n",
    "    df_list = []\n",
    "    for ui in item_list:\n",
    "        if 'eo:cloud_cover' in ui.properties:\n",
    "            # print('eo:cloud_cover in item properties')\n",
    "            cloud_cover = ui.properties['eo:cloud_cover']\n",
    "        else:\n",
    "            # print('eo:cloud_cover not in item properties, manually calculating')\n",
    "            cloud_cover = calculate_cloud_cover(rio.open(ui.get_assets()['CLD'].href).read())\n",
    "        uid = {\n",
    "            'item': ui,\n",
    "            'id': ui.id,\n",
    "            'cloud_cover': cloud_cover,\n",
    "            'date_time': ui.datetime\n",
    "        }\n",
    "        df_list.append(uid)\n",
    "        \n",
    "    cc_df = pd.DataFrame(df_list)\n",
    "    \n",
    "    # filters source items by cloud cover rank and returns ItemCollection\n",
    "    if not cc_df.empty:\n",
    "        filtered_df = filter_quarter_items(cc_df)\n",
    "        \n",
    "        return ItemCollection(filtered_df['item'].tolist())\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b3ad2c5-0ea6-47a0-afe0-6e305a68720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_item_collection(label_item: Item) -> ItemCollection:\n",
    "    \n",
    "    \"\"\"Takes a label Item from the LandCoverNet Collection and searches\n",
    "    for source imagery for chips that match spatial and temporal criteria\n",
    "    \n",
    "    Args:\n",
    "    label_item: Item - item of current iteration in the get_item() Dask parallelization\n",
    "    \n",
    "    Returns:\n",
    "    ItemCollection - STAC Iterable containing Items that match search criteria\n",
    "    \"\"\"\n",
    "    \n",
    "    n = 0\n",
    "    cc_thresh = 10\n",
    "    year_collection = ItemCollection([])\n",
    "    \n",
    "    # iterate over each start and end date per quarter\n",
    "    for start, end in quarter_ranges:\n",
    "    \n",
    "        while n == 0:\n",
    "\n",
    "            # performs a temporal and spatial search for each label item\n",
    "            search = mlhub_client.search(\n",
    "                collections=['ref_landcovernet_v1_source'],\n",
    "                intersects=mapping(shape(label_item.geometry)),\n",
    "                datetime=[start, end],\n",
    "                query={\"eo:cloud_cover\": {\"lt\": cc_thresh}},\n",
    "            )\n",
    "\n",
    "            # converts search results to ItemCollection\n",
    "            item_results = search.get_all_items()\n",
    "            # print(f'Search resulted in {len(item_results)} items between {start} and {end}')\n",
    "            \n",
    "            if not item_results:\n",
    "                # print(f'Search criteria for {label_item.id} using cloud cover threshold of {cc_thresh} between {start} and {end} did not return any source items')\n",
    "                cc_thresh += 5\n",
    "            else:\n",
    "                n = len(item_results)\n",
    "                \n",
    "        year_collection += item_results # concatenate ItemCollections for each quarter\n",
    "        n = 0 # reset the length criteria for search results\n",
    "        \n",
    "    filtered_items = get_season_min_cloud_cover(year_collection.items)\n",
    "    \n",
    "    return filtered_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "102fbbe5-4ed8-4c43-b7b9-2a29bac7c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code reference from https://github.com/TomAugspurger/esip-summer-2021-geospatial-ml/blob/main/segmentation-model.ipynb\n",
    "\n",
    "#This function will load source imagery and label into xarray for further processing\n",
    "def get_item(label_item: Item, assets: Tuple[str]) -> (np.ndarray, np.ndarray):\n",
    "    \n",
    "    \"\"\"Takes label Item and asset bands to construct n-darrays for model training\n",
    "    \n",
    "    Args:\n",
    "    label_item: Item - item of current iteration in the get_item() Dask parallelization\n",
    "    assets: Tuple[str] - a set of strings corresponding to the Asset band names\n",
    "    \n",
    "    Returns:\n",
    "    data: np.ndarray, labels: np.ndarray - X and y n-darrays for model training\n",
    "    \"\"\"\n",
    "    \n",
    "    assets = list(assets)\n",
    "    labels = rioxarray.open_rasterio(\n",
    "        tmp_dir+\"/landcovnet/labels/ref_landcovernet_v1_labels/\"+label_item.id+\"/labels.tif\",\n",
    "    ).squeeze()\n",
    "    \n",
    "    source_item_collection = get_label_item_collection(label_item)\n",
    "    \n",
    "    if len(source_item_collection) > 0:\n",
    "    \n",
    "        bounds = tuple(round(x, 0) for x in labels.rio.bounds())\n",
    "        \n",
    "        data = (\n",
    "                stackstac.stack(\n",
    "                    items=source_item_collection,\n",
    "                    assets=assets,\n",
    "                    dtype=\"float32\",\n",
    "                    resolution=10,\n",
    "                    bounds=bounds,\n",
    "                    epsg=labels.rio.crs.to_epsg(),\n",
    "                )\n",
    "            )\n",
    "            #assert data.shape[1:] == labels.shape\n",
    "        data = data.assign_coords(x=labels.x.data, y=labels.y.data)\n",
    "        data /= 4000\n",
    "        data = np.clip(data, 0,1)\n",
    "        \n",
    "        return data, labels.astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1173b773-3ce5-4de1-966b-b23243aa6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarter_ranges() -> List[List[str]]:\n",
    "    \n",
    "    \"\"\"Builds a list of start and end date ranges for each quarter in the year\n",
    "    \n",
    "    Args: None\n",
    "    Returns:\n",
    "    quarter_ranges: List[List[str]] - a list of pairs of strings representing the start and end dates\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    td = timedelta(days=1)\n",
    "    \n",
    "    quarter_ends = [pd.to_datetime(d) for d in pd.date_range(temporal_start, temporal_end, freq='Q').values]\n",
    "    quarter_ranges = []\n",
    "\n",
    "    for ix, quarter_end in enumerate(quarter_ends):\n",
    "        if ix == 0:\n",
    "            quarter_ranges.append([datetime.strptime(temporal_start, '%Y-%m-%d').strftime('%Y-%m-%d'), quarter_end.strftime('%Y-%m-%d')])\n",
    "            # print(datetime.strptime(temporal_start, '%Y-%m-%d'), quarter_end)\n",
    "        else:\n",
    "            quarter_ranges.append([(quarter_ends[ix-1] + td).strftime('%Y-%m-%d'), quarter_end.strftime('%Y-%m-%d')])\n",
    "            # print(quarter_ends[ix-1] + td, quarter_end)\n",
    "    return quarter_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7e515-d0b9-44be-a1f7-d9643edde701",
   "metadata": {},
   "source": [
    "#### Extracting Source Imagery\n",
    "\n",
    "Let's find the the source imagery associated with those labels by examining the Item links (source imagery links will have a `\"rel\"` type of `\"source\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12727fc1-67ed-4ee3-a9ff-87ce643576c3",
   "metadata": {},
   "source": [
    "#### Loading the source imagery from their respective links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebf56d39-8676-4a2b-8a42-12d91e777c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdda7a62-cd3a-41e4-ab52-d0ba985b4d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain bounding boxes for each label\n",
    "temporal_start = catalog.extent.temporal.intervals[0][0].strftime(\"%Y-%m-%d\") # global starting datetime for label Collection\n",
    "temporal_end = catalog.extent.temporal.intervals[0][1].strftime(\"%Y-%m-%d\") # global ending datetime for label Collection\n",
    "quarter_ranges = get_quarter_ranges()\n",
    "assets = (\"B04\", \"B03\", \"B02\") # we will make use of the RGB bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c255ac3-223e-4fc5-a480-1a836865fb72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "Xys_list=[]\n",
    "\n",
    "chunk_size = 20\n",
    "for i in range(0, len(label_items), chunk_size):\n",
    "    label_chunk=label_items[i:i+chunk_size]\n",
    "\n",
    "    Xys=[]\n",
    "    get_item_ = dask.delayed(get_item, nout=5)\n",
    "\n",
    "    Xys.append([get_item_(label, assets) for label in label_chunk])\n",
    "    Xys = dask.persist(*Xys)\n",
    "    Xys = dask.compute(*Xys)\n",
    "    Xys_list.append(Xys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cb07ea9-8e36-43fd-92d4-0d072aab1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca0d6014-40e8-4441-9b80-9905d3dd5f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "flat_list = [item for sublist in Xys_list for item in sublist]\n",
    "#pickle.dump((flat_list), open(f'{tmp_dir}/landcovnet/items' + '.pkl', 'ab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06e349c3-3ece-4fd6-b800-5ac360dd874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((flat_list), open(f'{tmp_dir}/landcovnet/items' + '.pkl', 'ab'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
